# @package _global_

hydra:
  launcher:
    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher

train:
  seed: 42
  split_seed: 42
  output_dir: "./outputs/dpo_kl/${exp.loss_type}/beta:${exp.loss_alpha}-gamma:${exp.tol}"
  do_train: true
  do_initial_eval: false
  use_wandb: true
  save_model: true
  push_adapters_to_wandb: true
  wandb_project: "dpo_kl"
  run_name: "dpo_kl-${exp.loss_type}-beta:${exp.loss_alpha}-gamma:${exp.tol}"

  cache_dir: "./cache"
  val_fraction: 0.1
  shuffle: true
  sanity_check: false
  data_proportion: 1.0
  max_prompt_length: 1024
  max_length: 1536
  max_gen: 10

  lora:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05

  hf_args:
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 2
    eval_accumulation_steps: 4
    num_train_epochs: 20
    learning_rate: 1e-5
    lr_scheduler_type: "cosine"
    warmup_steps: 200
    weight_decay: 0.0
    eval_strategy: "no"
    save_strategy: "no"
    gradient_checkpointing: false
    eval_do_concat_batches: false
    bf16: true
    fp16: false

exp:
  name: "dpo_kl"
  model_name: meta-llama/Llama-3.1-8B-Instruct

  # SimPO margin γ (reusing the constraint tolerance field)
  tol: 1.0

  loss_type: simpo

  # SimPO temperature / scaling (β)
  loss_alpha: 2.0
  dual_step_size: 1.0

